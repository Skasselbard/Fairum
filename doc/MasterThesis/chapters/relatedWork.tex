% !TEX root = ../main.tex
\chapter{Background and related work}
%TODO: intro
Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\cite{klabnik2018rust}
\section{Rust}
\label{rel_rust}
Typically programming languages can be divided into fast or safe: 
To increase software speed, developers have to optimize specific system details.
This micro management easily leads to mistakes as it is hard to keep track of the complexity of the whole system.
Mistakes that can be avoided with safety features like garbage collection -- compromising execution speed.

The Rust project is an attempt to build a language that is both: fast and safe, 
as the official slogan indicates: ``A language empowering everyone
to build reliable and efficient software.''\cite{rustSite}.
%Among Rusts design considerations were memory safety, concurrency and parallelism\cite{Matsakis:2014:RL:2692956.2663188}.
To ensure a minimal performance overhead Rusts runtime was kept small\cite[Chapter 16.1]{klabnik2018rust} and features like garbage collection where neglected\cite[Chapter 4]{klabnik2018rust}.
Instead, safety issues are addressed with Rusts \textbf{ownership model}\cite{Matsakis:2014:RL:2692956.2663188}.

In Rust, a memory resource (object) is associated by one unique owning variable.
The owner can mutate the object, reference it or hand over ownership.
By handing over ownership it is lost for the previous owner, to unsure unique ownership.
However, references can be `borrowed' in two flavors without loosing ownership:
\begin{enumerate}
  \item There can be an arbitrary amount of \textbf{immutable references} at a given time.
  \item But only one active \textbf{mutable reference}. 
  No immutable references can be active at the same time and the owner is prohibited from mutating.
\end{enumerate}
References that go out of scope are ensured to be deconstructed by Rusts \textbf{borrow checker}. Programs that don't meet the ownership requirements will not compile and raise an appropriate error message.

By enforcing the ownership rules, Rust programs avoid common problems like dangling pointers, double frees and data races\cite{Matsakis:2014:RL:2692956.2663188} with no impact on execution speed.
The cost is transferred to compile time, where additional errors complicate development and established programing patterns have to be revised.
And while eliminating all these errors can be invaluable, Rust cannot prevent all mistakes.
One of which are deadlocks\cite[Chapter 8.1]{nomicon} which we want to address with this work.

\section{Compilers}
The goal of this work is to combine the benefits of the Rust ownership system with the benefits of Petri-Net model checking.

To achieve this goal we have to translate from Rust to Petri-Nets, 
and we want to do it programmatically.
This is basically the definition of a compiler\cite[Chapter 1.1]{aho1986compilers}:
\begin{quote}
``Simply stated, a compiler is a program that can read a program in one language -- the source language -- and translate it into an equivalent program in another language -- the target language;''
\end{quote}

Compilers underwent heavy research and development in the past.
Nowadays the structure of a compiler can be summarized into well defined phases\cite[Chapter 1.2]{aho1986compilers}:

\begin{enumerate}
  \item During the \textbf{Lexical Analysis}, the character stream of a source file is converted into a token stream.
  Tokens are all significant language components like keywords, identifiers and symbols (`=', `+', `\{', etc.).
  \item During \textbf{Syntax Analysis} (parsing) the token stream is structured into a tree,
  typically a syntax tree, where each node represents an operation with its children as operation arguments.
  \item The following \textbf{Semantic Analysis} checks that the syntax actually matches the requirements;
  The grammar that the language is based on.\newline
  Additional static analysis -- like type checking -- is done in this phase as well.
  \item Further representations might be produced in the \textbf{Intermediate Code Generation} phase.
  An intermediate representation can be everything that helps.
  A low level representation that is close to machine code is a common case.
  Examples are Java Bytecode or the LLVM intermediate representation
  \item The intermediate representation can be used for further analysis and optimization in the \textbf{Code Optimization} phase.
  Executable size or execution speed might be improved here.
  Multiple intermediate representations might be generated and optimized before entering the final phase.
  \item \textbf{Code Generation};
  Which generates another representation.
  The only difference is that it is the final one -- the target representation.
  Thus it often produces executable machine code.
\end{enumerate}

These phases should clarify the general concept of a compiler but in practice phases might be less distinct.
They can blend together and some can be skipped entirely.
In the end however, we have a mapping from the source representation to the target representation.

\section{Parallel Programs}
\label{rel_para}
\begin{verbatim}
- the problem with parallel programs
  - need to communicate data
    - messages
    - shared memory
  - data needs to be 
    - consistent 
    - synchronized (wait for each other)
    - up to date
  - deadlocks can easily be introduced with 
    synchronisation flaws \cite{}
- what are deadlocks
  - synchronisation
  - mutex/semaphore
  - threads
  - dining philosophers
- rust and parallel programs
- how can deadlocks be introduced in rust
  - rust and deadlocks -> considered safe code
\end{verbatim}

\section{Model Checking}
\label{rel_mc}
\begin{verbatim}
- tests vs Model Checking
  - tests 
    - only work for specific cases\cite{}
    - done by program execution
  - Model Checking 
    - works in the general case\cite{}
    - done by program analysis
- different approaches (BDDs etc.)
- petri nets!!
\end{verbatim}

\subsection{Petri-Nets}
\label{rel_petri}
\begin{verbatim}
- 
\end{verbatim}

\subsection{CTL*}
\label{rel_ctl}
\begin{verbatim}
- 
\end{verbatim}


\begin{verbatim}
  - other verification implementations
    - (verification by language?)
      - functional programming invariants?
      - prolog invariants?
      - languages with verification methods in its design?
    - c verification
      - valgrind?
    - rust verification
  - petri net verification
    - bpel
\end{verbatim}
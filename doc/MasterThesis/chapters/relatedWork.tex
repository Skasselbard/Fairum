% !TEX root = ../main.tex
\chapter{Background and related work}
%TODO: intro
Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\cite{klabnik2018rust}
\section{Rust}
\label{rel_rust}
\begin{verbatim}
- ~70% of the vulnerabilities Microsoft assigns a CVE each year 
  continue to be memory safety issues
  \cite{gavin2019secureCode}\cite{miller2019trends}
- rust = fast and safe
- rust empowers bla bla \cite{rustSite}
- designed to support concurrency and parallelism\cite{Matsakis:2014:RL:2692956.2663188}
- low level with high level zero cost abstractions\cite[Introduction]{klabnik2018rust}
- small runtime\cite[Chapter 16.1]{klabnik2018rust}
- no garbage collection\cite[Chapter 4]{klabnik2018rust}
- instead ownership model\cite{Matsakis:2014:RL:2692956.2663188}
  - controlled memory access
    - ownership
      - owner of object is unique
      - ownership can be handed over
      - references to/into object can be borrowed
        - if borrow goes out of scope -> no references remain
          -> no dangling pointers
        - arbitrary amount of immutable borrowed references
        - or at most one active mutable borrowed reference
  - no dangling pointers, double frees, data races\cite{Matsakis:2014:RL:2692956.2663188}
  - fast vs safe vs compile time effort \cite{speedSafety}
  - reduced expressiveness in exchange for increased memory safety
- deadlocks are considered safe in rust terms \cite[Chapter 8.1]{nomicon}
\end{verbatim}

\section{Compilers}
The goal of this work is to combine the benefits of the Rust ownership system with the benefits of Petri-Net model checking.

To achieve this goal we have to translate from Rust to Petri-Nets, 
and we want to do it programmatically.
This is basically the definition of a compiler\cite[Chapter 1.1]{aho1986compilers}:
\begin{quote}
``Simply stated, a compiler is a program that can read a program in one language -- the source language -- and translate it into an equivalent program in another language -- the target language;''
\end{quote}

Compilers underwent heavy research and development in the past.
Nowadays the structure of a compiler can be summarized into well defined phases\cite[Chapter 1.2]{aho1986compilers}:

\begin{enumerate}
  \item During the \textbf{Lexical Analysis}, the character stream of a source file is converted into a token stream.
  Tokens are all significant language components like keywords, identifiers and symbols (`=', `+', `\{', etc.).
  \item During \textbf{Syntax Analysis} (parsing) the token stream is structured into a tree,
  typically a syntax tree, where each node represents an operation with its children as operation arguments.
  \item The following \textbf{Semantic Analysis} checks that the syntax actually matches the requirements;
  The grammar that the language is based on.\newline
  Additional static analysis -- like type checking -- is done in this phase as well.
  \item Further representations might be produced in the \textbf{Intermediate Code Generation} phase.
  An intermediate representation can be everything that helps.
  A low level representation that is close to machine code is a common case.
  Examples are Java Bytecode or the LLVM intermediate representation
  \item The intermediate representation can be used for further analysis and optimization in the \textbf{Code Optimization} phase.
  Executable size or execution speed might be improved here.
  Multiple intermediate representations might be generated and optimized before entering the final phase.
  \item \textbf{Code Generation};
  Which generates another representation.
  The only difference is that it is the final one -- the target representation.
  Thus it often produces executable machine code.
\end{enumerate}

These phases should clarify the general concept of a compiler but in practice phases might be less distinct.
They can blend together and some can be skipped entirely.
In the end however, we have a mapping from the source representation to the target representation.

\section{Parallel Programs}
\label{rel_para}
\begin{verbatim}
- the problem with parallel programs
  - need to communicate data
    - messages
    - shared memory
  - data needs to be 
    - consistent 
    - synchronized (wait for each other)
    - up to date
  - deadlocks can easily be introduced with 
    synchronisation flaws \cite{}
- what are deadlocks
  - synchronisation
  - mutex/semaphore
  - threads
  - dining philosophers
- rust and parallel programs
- how can deadlocks be introduced in rust
  - rust and deadlocks -> considered safe code
\end{verbatim}

\section{Model Checking}
\label{rel_mc}
\begin{verbatim}
- tests vs Model Checking
  - tests 
    - only work for specific cases\cite{}
    - done by program execution
  - Model Checking 
    - works in the general case\cite{}
    - done by program analysis
- different approaches (BDDs etc.)
- petri nets!!
\end{verbatim}

\subsection{Petri-Nets}
\label{rel_petri}
\begin{verbatim}
- 
\end{verbatim}

\subsection{CTL*}
\label{rel_ctl}
\begin{verbatim}
- 
\end{verbatim}


\begin{verbatim}
  - other verification implementations
    - (verification by language?)
      - functional programming invariants?
      - prolog invariants?
      - languages with verification methods in its design?
    - c verification
      - valgrind?
    - rust verification
  - petri net verification
    - bpel
\end{verbatim}
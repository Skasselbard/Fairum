% !TEX root = ../main.tex
\chapter{Background and related work}
\section{Rust}
\label{rel_rust}
\begin{verbatim}
- rust empowers bla bla \cite{}
- low level with high level abstractions
- small runtime
- no garbage collection
- controlled memory access
  - reduced expressiveness in exchange for increased memory safety
  - features to tackle memory safety issues
  - borrow checker
  - no data races etc
- fast vs safe vs compile time effort \cite{}
- deadlocks are considered safe in rust terms \cite{}
\end{verbatim}

\section{Compilers}
The goal of this work is to combine the benefits of the Rust ownership system with the benefits of Petri-Net model checking.

To achieve this goal we have to translate from Rust to Petri-Nets, 
and we want to do it programmatically.
This is basically the definition of a compiler\cite[Chapter 1.1]{aho1986compilers}:
\begin{quote}
``Simply stated, a compiler is a program that can read a program in one language -- the source language -- and translate it into an equivalent program in another language -- the target language;''
\end{quote}

Compilers underwent heavy research and development in the past.
Nowadays the structure of a compiler can be summarized into well defined phases\cite[Chapter 1.2]{aho1986compilers}:

\begin{enumerate}
  \item During the \textbf{Lexical Analysis}, the character stream of a source file is converted into a token stream.
  Tokens are all significant language components like keywords, identifiers and symbols (`=', `+', `\{', etc.).
  \item During \textbf{Syntax Analysis} (parsing) the token stream is structured into a tree,
  typically a syntax tree, where each node represents an operation with its children as operation arguments.
  \item The following \textbf{Semantic Analysis} checks that the syntax actually matches the requirements;
  The grammar that the language is based on.\newline
  Additional static analysis -- like type checking -- is done in this phase as well.
  \item Further representations might be produced in the \textbf{Intermediate Code Generation} phase.
  An intermediate representation can be everything that helps.
  A low level representation that is close to machine code is a common case.
  Examples are Java Bytecode or the LLVM intermediate representation
  \item The intermediate representation can be used for further analysis and optimization in the \textbf{Code Optimization} phase.
  Executable size or execution speed might be improved here.
  Multiple intermediate representations might be generated and optimized before entering the final phase.
  \item \textbf{Code Generation};
  Which generates another representation.
  The only difference is that it is the final one -- the target representation.
  Thus it often produces executable machine code.
\end{enumerate}

These phases should clarify the general concept of a compiler but in practice phases might be less distinct.
They can blend together and some can be skipped entirely.
In the end however, we have a mapping from the source representation to the target representation.

\section{Parallel Programs}
\label{rel_para}
\begin{verbatim}
- the problem with parallel programs
  - need to communicate data
    - messages
    - shared memory
  - data needs to be 
    - consistent 
    - synchronized (wait for each other)
    - up to date
  - deadlocks can easily be introduced with 
    synchronisation flaws \cite{}
- what are deadlocks
  - synchronisation
  - mutex/semaphore
  - threads
  - dining philosophers
- rust and parallel programs
- how can deadlocks be introduced in rust
  - rust and deadlocks -> considered safe code
\end{verbatim}

\section{Model Checking}
\label{rel_mc}
\begin{verbatim}
- tests vs Model Checking
  - tests 
    - only work for specific cases\cite{}
    - done by program execution
  - Model Checking 
    - works in the general case\cite{}
    - done by program analysis
- different approaches (BDDs etc.)
- petri nets!!
\end{verbatim}

\subsection{Petri-Nets}
\label{rel_petri}
\begin{verbatim}
- 
\end{verbatim}

\subsection{CTL*}
\label{rel_ctl}
\begin{verbatim}
- 
\end{verbatim}


\begin{verbatim}
  - other verification implementations
    - (verification by language?)
      - functional programming invariants?
      - prolog invariants?
      - languages with verification methods in its design?
    - c verification
      - valgrind?
    - rust verification
  - petri net verification
    - bpel
\end{verbatim}
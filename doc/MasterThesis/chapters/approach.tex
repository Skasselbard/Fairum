% !TEX root = ../main.tex


\chapter{Approach}
After we learned the underlying concepts, we can go on with the actual task.

In this chapter it is shown how the initial rust code is translated and analysed.
We will see that the rust compiler has interfaces that we can use to reduce our effort,
and that it creates an intermediate representation that fits our needs nicely.
We will work out a translation for the elements of this intermediate representation, 
and discuss the format that we will translate into.
Finally we will discuss how we can use a model checker to find deadlocks in some simple test programs.
%TODO: Ã¼berleitungssatz?

\section{Rust Compiler}
\label{app_rust}
There are basically two options to translate rust into Petri-Nets:
\begin{enumerate}
    \item write an own translator or,
    \item use something existing.
\end{enumerate}
Writing an own translator means total control over the process.
Features can be added iteratively as needed and data structures can be designed efficiently for this special perpose.
However, this would result in a new compiler for rust which eventually has to cover every language feature;
Including some difficult ones like macro expansion and generics.
Features that someone already implemented, spending a fair amount of thought in the process;
Backed by a large community;
Over several years.
Resulting in a compiler that is openly available under an open source license\cite{rustc}, with a maintained documentation\cite{rustc-guide}\cite{rustc-doc}.
And even though learning how this complex compiler works, and where to find the relevant parts for this project is also difficult, it seems to be time worth spend if we are able to skip learning and implementing difficult features.
For this reason this project is based on the rust compiler `rustc'.
So lets see how its basic structure looks like.

The rust compiler does have several phases and a variety of intermediate representations \cite[Chapter 2.1]{rustc-guide}:
\begin{enumerate}
    \item In the first phase rust source files are parsed to an abstract syntax tree (\textbf{AST}) that matches the original syntax closely.
    \item In the second phase implicit information is expanded. This includes for example macros and identifier names.
    \item Phase three lowers the AST to a simpler form called high-level intermediate representation or \textbf{HIR}.
    The HIR still is quite similar to normal rust syntax but some structures are normalised so that code analysis is easier. 
    For example for loops are rewritten to simple endless loops with break conditions.
    \item The fourth phase executes some static analysis on the HIR.
    Things like type checking and encapsulation verification is done on this representation.
    \item Another representation is generated in phase five: the mid-level intermediate representation (\textbf{MIR}).
    Now we leaving the tree structure and switching to a graph.
    MIR is based on a control-flow graph \cite{} and language features are reduced to a minimum.
    There is only one type of loop and branches respectively (only gotos instead of ifs or pattern matching).
    Additional static analysis is done on the mir level,
    like rusts borrow checking, as well as further optimization.
    \item Phase six lowers the MIR to the rust independant LLVM\cite{} intermediate representation.
    LLVM IR is a low level representation that is close to assembly language.
    Optimization for this representation can now be done for the code resulting in several object files.
    \item In the last phase the object files are linked into a complete binary executable.
\end{enumerate}
But wich phase is best to intercept to translate the current intermediate representation into petri nets?

\section{Interception strategie}
\label{app_intercept}
After deciding to use the rust compiler as basis for this work, we need to determine the phase we want to intercept the default compilation to translate to our own target.
A suitable location makes use of the most possible compiler features with the least amount of translation effort.

We want to skip basic features like name resolution as we need this feature ourselfs and surely won't improve it by a reimplementation.
Also code generation, including macro and generics expansion, should be handled by the compiler.
They just produce more rust code that we will treat anyway.
But after expansion, no code generation syntax will remain and we can simply ignore this feature.

More optional for our use are the static analysis features like type checking and borrow checking.
Though, we want to use their assumption, we do not depend on them actually being checked, since nobody will ever run a rust program that did not go through the complete compilation process.
So it is quite safe to assume that nobody will do model checking on a program that won't compile.
However, we still can enforce those invariants if we run the static analisis anyway and abort on errors.
This prevents time consuming runs for programs that cannot be build.

A compiler feature that \textit{can} greatly ease our effort is the lowering of the representation.
After all that means that several high level language features are reduced to less low level features.
Which in turn means less features we have to cover with our implementation.
We don't want to go to low though, since lower representations might prevent us from exploiting assumptions that are hidden in the translation process.
We also probably want to avoid a machine dependant representation.
To verify generic rust programs and not the peculiarities of a single machine.
This might be debateable though.

Lastly we want to make use of all optimizations we can.
Especially optimizations that reduce the code (and resulting net) size.
So features like constant propagation and dead code elimination would be nice to have to reduce the limit of state explosion.
At least a little bit.

Looking back an the rust compiler phases with those thoughts in mind, the seemingly best place we can intercept is between phase five and six.
After borrow checking and optimizations on the MIR.
Here, all code was expended, all rust specific static analisis and optimization was done and all unnecessary language features where reduced to a minimal set of instructions.
And since MIR is a graph representation as well as petri nets, a mapping should be relatively straight forward.
More so if we consider that MIR represents control flow quite closely as petri nets do.
Also we avoid the still lower level and rust independent LLVM IR and the machine dependent object files. 

\section{Mid-level Intermediate Representation (MIR)}
\label{app_mir}
\begin{verbatim}
- control flow graph

other stuff from rustc book

      - how to traverse
      - single elements
\end{verbatim}

\section{Petri Net Representation}
\label{app_petri}
\begin{verbatim}
    - petri net formalism and format (high level? edge descriptions)
\end{verbatim}

\section{Translation}
\label{app_trans}
\begin{verbatim}
- entry point
    - using main
    - ignoring pre main
        - os specific
        - independent from program semantics
        - not all mir available
- elements
    - programm
        - starts
        - ends
    - memory
        - constants
        - statics
        - places
        - locals
    - function
        - no translation memory (multiple calls = multiple translations)
        - stack like structure
        - no recursion possible with this approach
        - can panic
        - can diverge
        - can be empty (in theorie)
    - basic blocks
    - statement
        - rvalues and operands
        - assign
        - etc
    - terminator
        - goto
        - switch int
        - call
        - etc
- panic handling
    - do not catch by other threads
    - distinguish between successful and unsuccessful termination
- which parts can or must be excluded from translation
    - std implementations
    - part between std::mutex and pthread mutex
- emulating features
    - missing mir parts
    - external libraries
    - intrinsics and platform specific behavior
    - mutexes
        - high level and low level interfaces
        - choosing high level
            - harder model checking for more general Mutexes
            - os independent
            - less elements need to be translated
                - smaller net
                - worse mapping
        - need to track associated locals

- joining the translations
    - joining basic elements
    - joining basic blocks
    - joining function calls
    - recursion limits
\end{verbatim}


\section{Model Checking}
\label{app_mc}
\begin{verbatim}
- many tools
- pnml as common representation language
    - finding tools on standard site
- lola integration (format of a petri net)
  - own format
- CTL formula
    - expected deadlocks (program end)
    - difference between deadlock in petri net and deadlock in program
        - try to stay analogous
    - difference between p = 0 and EF( p = 0 )
    - try to force witness paths
\end{verbatim}

\section{Test Programs}
\label{app_test}
\begin{verbatim}
- stay simple to prove concept
    - empty program
    - programs which cover the basic language features
    - program with a deadlock
    - similar program without a deadlock
\end{verbatim}

\section{Debugging}
\label{app_debug}
\begin{verbatim}
- dot file for small programms
- finding unconnected nodes
    - possible bug found
    - marking live places as result
    - uninitialized places still have to be marked
      (or removed entirely)
- witness path
    - reducing the complicated net to witness path nodes 
      and its neighbors
\end{verbatim}
% !TEX root = ../main.tex


\chapter{Approach}
After we learned the underlying concepts, we can go on with the actual task.

In this chapter it is shown how the initial rust code is translated and analysed.
We will see that the rust compiler has interfaces that we can use to reduce our effort,
and that it creates an intermediate representation that fits our needs nicely.
We will work out a translation for the elements of this intermediate representation, 
and discuss the format that we will translate into.
Finally we will discuss how we can use a model checker to find deadlocks in some simple test programs.
%TODO: Ã¼berleitungssatz?

\section{Rust Compiler}
\label{app_rust}
There are basically two options to translate rust into Petri-Nets:
\begin{enumerate}
    \item write an own translator or,
    \item use something existing.
\end{enumerate}
Writing an own translator means total control over the process.
Features can be added iteratively as needed and data structures can be designed efficiently for this special perpose.
However, this would result in a new compiler for rust which eventually has to cover every language feature;
Including some difficult ones like macro expansion and generics.
Features that someone already implemented, spending a fair amount of thought in the process;
Backed by a large community;
Over several years.
Resulting in a compiler that is openly available under an open source license\cite{rustc}, with a maintained documentation\cite{rustc-guide}\cite{rustc-doc}.
And even though learning how this complex compiler works, and where to find the relevant parts for this project is also difficult, it seems to be time worth spend if we are able to skip learning and implementing difficult features.
For this reason this project is based on the rust compiler `rustc'.
So lets see how its basic structure looks like.

The rust compiler does have several phases and a variety of intermediate representations \cite[Chapter 2.1]{rustc-guide}:
\begin{enumerate}
    \item In the first phase rust source files are parsed to an abstract syntax tree (\textbf{AST}) that matches the original syntax closely.
    \item In the second phase implicit information is expanded. This includes for example macros and identifier names.
    \item Phase three lowers the AST to a simpler form called high-level intermediate representation or \textbf{HIR}.
    The HIR still is quite similar to normal rust syntax but some structures are normalised so that code analysis is easier. 
    For example for loops are rewritten to simple endless loops with break conditions.
    \item The fourth phase executes some static analysis on the HIR.
    Things like type checking and encapsulation verification is done on this representation.
    \item Another representation is generated in phase five: the mid-level intermediate representation (\textbf{MIR}).
    Now we leaving the tree structure and switching to a graph.
    MIR is based on a control-flow graph \cite{} and language features are reduced to a minimum.
    There is only one type of loop and branches respectively (only gotos instead of ifs or pattern matching).
    Additional static analysis is done on the mir level,
    like rusts borrow checking, as well as further optimization.
    \item Phase six lowers the MIR to the rust independant LLVM\cite{} intermediate representation.
    LLVM IR is a low level representation that is close to assembly language.
    Optimization for this representation can now be done for the code resulting in several object files.
    \item In the last phase the object files are linked into a complete binary executable.
\end{enumerate}
But wich phase is best to intercept to translate the current intermediate representation into petri nets?

\section{Interception strategie}
\label{app_intercept}
After deciding to use the rust compiler as basis for this work, we need to determine the phase we want to intercept the default compilation to translate to our own target.
A suitable location makes use of the most possible compiler features with the least amount of translation effort.

We want to skip basic features like name resolution as we need this feature ourselfs and surely won't improve it by a reimplementation.
Also code generation, including macro and generics expansion, should be handled by the compiler.
They just produce more rust code that we will treat anyway.
But after expansion, no code generation syntax will remain and we can simply ignore this feature.

More optional for our use are the static analysis features like type checking and borrow checking.
Though, we want to use their assumption, we do not depend on them actually being checked, since nobody will ever run a rust program that did not go through the complete compilation process.
So it is quite safe to assume that nobody will do model checking on a program that won't compile.
However, we still can enforce those invariants if we run the static analisis anyway and abort on errors.
This prevents time consuming runs for programs that cannot be build.

A compiler feature that \textit{can} greatly ease our effort is the lowering of the representation.
After all that means that several high level language features are reduced to less low level features.
Which in turn means less features we have to cover with our implementation.
We don't want to go to low though, since lower representations might prevent us from exploiting assumptions that are hidden in the translation process.
We also probably want to avoid a machine dependant representation.
To verify generic rust programs and not the peculiarities of a single machine.
This might be debateable though.

Lastly we want to make use of all optimizations we can.
Especially optimizations that reduce the code (and resulting net) size.
So features like constant propagation and dead code elimination would be nice to have to reduce the limit of state explosion.
At least a little bit.

Looking back an the rust compiler phases with those thoughts in mind, the seemingly best place we can intercept is between phase five and six.
After borrow checking and optimizations on the MIR.
Here, all code was expended, all rust specific static analisis and optimization was done and all unnecessary language features where reduced to a minimal set of instructions.
And since MIR is a graph representation as well as petri nets, a mapping should be relatively straight forward.
More so if we consider that MIR represents control flow quite closely as petri nets do.
Also we avoid the still lower level and rust independent LLVM IR and the machine dependent object files. 

\section{Mid-level Intermediate Representation (MIR)}
\label{app_mir}
%TODO: examples examples examples

Now where we pinned down MIR as the intermediate representation to use, it is helpful to understand how it is structured.

\begin{figure}
    \centering
    \includegraphics[width=.7\textwidth]{../diagrams/BasicBlock.png}
    \caption{
        Structure of a basic block. 
        Several chained statements with a single trailing terminator.
        The terminator can redirect control flow to other basic blocks.
        In this example BB3 branches to BB4 or BB7 depending on the value of local8
        }
    \label{mir_bb}
\end{figure}

As mentioned before, MIR is derived from control flow graphs\cite[chapter 2.17]{rustc-guide}.
It consists of several \textbf{basic blocks} which are interconnected with directed edges.
Each basic block consist of any amount of non branching \textbf{statements} and a single -- possibly branching -- \textbf{terminator}.
All statements in a single basic block are executed sequentially.
Between those, no branching can occur in or out.
Only terminators can redirect the control flow.
They are the ones that represents conditional execution (i.e. if-then-else constructs) or jumps to other basic blocks (including loops).

% \lstset{language=C++,caption={Basic implementation of a spinlock with compare and swap},label=CASIMPL, frame=none, stepnumber=5, backgroundcolor=\color{verylightgray}}
% \begin{lstlisting}
% #include <atomic>

% enum LOCK{
%     LOCKED,
%     UNLOCKED
% };

% static inline void waitAndLock(std::atomic<bool>* lock){
%     bool expected = UNLOCKED;
%     while (!lock->compare_exchange_weak(expected,LOCKED)){
%         expected = UNLOCKED;
%     }
% }

% static inline void waitForUnlock(std::atomic<bool>* lock){
%     while (lock->load() != UNLOCKED){
%         continue;
%     }
% }

% static inline void lock(std::atomic<bool>* lock){
%     lock->store(LOCKED);
% }

% static inline void unlock(std::atomic<bool>* lock){
%     lock->store(UNLOCKED);
% }
% \end{lstlisting}

Data in MIR is represented as \textbf{locals} and \textbf{places} (not to be confused with petri-net places).
Places represent any kind of memory location, whereas locations are conceptually located on the stack.
This means that a location can also be represented as a place, but places are not necessarily locations.
Statements work on these data representations.
The most prominent type of statement is an assignment that assigns an \textbf{rvalue} derived from an expression to a memory location -- meaning a place.
Also based on the data terminators can direct control flow.
Which branch the program takes might depend on the value a place currently holds.

\begin{figure}
    \centering
    \includegraphics[width=.7\textwidth]{../diagrams/Function.png}
    \caption{
        Example of a functions internal control flow.
        The statements of the basic blocks are omitted as they don't affect control flow.
        A function has a list of locals that are accessible on the stack.
        Each local has an associated type.
        Functions always start with the first basic block (BB0 in this case).
        This function either returns to the calling function in BB2
        or calls `funtion2' in BB3.
        When function2 returns, control flow continues in BB1.}
    \label{mir_fn}
\end{figure}

It is also important to know that each function has a separate MIR representation.
Calling other functions is an action done by a call-terminator.
They can direct control flow to a subroutine that is executed until a return-terminator is hit.
After that the calling function resumes execution at a previously defined basic block.
An important note is that the functions in a call are referenced by name and not with an edge in the graph (this results in the separate MIR representation for each function).
This approach ensures that recursive function calls are kept simple.

Besides the specific kinds of statements and terminators, this is the general concept for the MIR graph.
A more detailed view would be of little help since it would get very close to the actual implementation.
Implementation details, however, are frequently changing;
In fact during this work it changed multiple times breaking the code.
A circumstance that is communicated by the compiler team and needed because a too stable API would restrain the compiler development process.


\section{Translation}
\label{app_trans}
After we saw how the MIR graph is structured, we can try to find a translation to Petri-Nets. This will include some more details and edge case we have to consider.

\subsection{Entry Point}
The will start with the most abstract view on our translation: the whole program.
A program is something that typically has a beginning end an ending.
We can model this with a \textbf{program start} and an \textbf{program end} place that every program has.
Depending on the program they are interconnected somehow.
The sole exception are programs with a diverging main function -- where the main function ends in an endless loop.
In this case start und end place will not be connected, but an unconnected end place does not harm either.
In rust a second end place for panics -- the \textbf{panic end} place -- is a helpful addition.
This place will be marked then the program terminated unsuccessfully after a panic was raised.
A circumstance that might be helpful to distinguishing in verification runs.
Finally the program start place needs to be marked with a token.
We will later see that this token also indicates the first statement to execute and that it virtually moves from statement to statement acting like a program counter (even though petri net tokens do not move semantically but rather be consumed and produced).

Although these are all the basic features shared by every program, we still have to look a little closer on the semantics of the program start place.
This place is a bit ambiguous since, in practice, the main function is not wat is executed first.
Usually programs have a `runtime' that is initialized before main() is called.
These setup static memory and initialize language specific features, among other things.
And even though low level languages like rust or c hove a small runtime, they still have one.
Now we have to decide if this runtime should be considered for Petri-Net translation.
After all it is part of the finally executed binary.
On the other hand it is platform dependent code that is independant from the actual program semantics.
And there is another problem in starting in the pre main code.
It turns out that the MIR for that part is not completely available in every compiler version.
It is possible to get the missing parts subsequently but it complicates the translation process unnecessarily.
Additionally, we previously argued for a platform independent approach already in chapter \ref{app_intercept}.
It would be inconsistent to detach from that agenda now.
So, because of these reasons we decided to skip the pre main() code and start translating programs with the actual main method.

\begin{figure}
    \centering
    \includegraphics[width=.9\textwidth]{../diagrams/basic_program.png}
    \caption{
        Stubs for programs and functions, and a minimal terminating program.
        A program has a start and an end place.
        The start place holds a token, as program execution begins there.
        If a program terminates successfully the program end place is marked.
        A termination caused by a panic marks only the panic place.\\
        A function has a distinct starting place (it always starts at the first basic block).
        But there is no end place, as functions can be left by any terminator.\\
        The shortest possible (terminating) program has an empty main function.
        Program start place and main function start place are one and the same place.
        Empty functions have no basic block in MIR, so we have to add an artificial transition to leave the function.
    }
    \label{program_blocks_trans}
\end{figure}

\subsection{Functions}
After we modeled the important features of a program, we can look on the next important path that a program consists of.
Control flow wise these are functions.
As in most other imperative languages rust programs have a main function that wrap all its functionality (excluding the runtime as discussed in the previous section).
The main function can call other functions that inturn can call additional functions, and so on.
When executing a program the called functions are organized on a stack called `call stack'.
When a new function is entered a new stack frame is pushed on the stack including information like function arguments and local variables.
On leaving a function it is removed or `poped' from the stack deallocating memory the frame occupied.

In MIR calling and returning from functions is done in a basic block terminator and can occur arbitrarily often.
Considering this behavior in a function model would be of no help, leaving just the start place as a feasible similarity between functions.
A function that is called always begins its execution there.
And it will always be identical to the start place of the first basic block.

So, from a modeling perspective functions are not the decisive abstraction.
But if we consider the translation process they get more important.
The rustc interface for MIR works per function.
This means that it is not possible (at least no obvious one) to get the MIR from a whole program but only from single functions.
A likely explanation for this design choice is that a lot of context information switches with functions.
For example every functions starts with basic block zero and increases the count for the following.
Local variables are also indexed with some of them having a special meaning.
The first being the functions return value followed by variables for all function arguments.
If we want to translate a program we have to keep this structure in mind.
In our implementation we have done this virtually by an own call stack.
We traverse the program function by function.
Every time we encounter a function call terminator we try to translate the called function and return wre we left if we are done.
On the new call the context is switched to the new variables and basic blocks, and the return semantics is stored.

However, this approach has some implications:
\begin{enumerate}
    \item If the same function is called multiple times in the program, it is also translated multiple times.
    Although, this can probably avoided with an intelligent function cache this approach is sufficient for a proof of concept.
    \item The far more extensive implication are the voided recursion capabilities.
    If we encounter a recursive function with this strategie, we will be trapt in an endless loop.
\end{enumerate}
Actually recursion is a feature that can never be achieved with low-level Petri-Nets, as it data model cannot be mapped correctly.
How often a recursive function is called depends on the data it is called with and cannot be known at compile time.
In normal program execution functions are just pushed on a new stack frame until it is resolved (the base case is called for all recursive calls) or the stack overflows.
Most relevant programs will be still executable.
But we cannot model this stack like behavior in low-level Petri-Nets because we cannot get additional memory.
All memory we have is that we know of at compile time.
And we cannot reuse the places of a function for different recursion levels as we can not distinguish the tokens from the recursion levels.
We could just remodel each recursion level again and again to a fixed recursion level.
However, this would impact the verification results, since a property might change for programs with different maximum recursion depths.

A solution to this problem can be high-level petri nets, where we can distinguish between tokens.
There we could reuse places from the same function by annotating tokens with the corresponding recursion level.
We won't go into detail of this approach, though, since this work is based on the low-level semantics.
We will just have to accept that we cannot translate recursive functions for the time being.

\subsection{Data}

\begin{verbatim}
- elements
    - memory
        - constants
        - statics
        - places
        - locals
    - function
        - can panic
        - can diverge
        - can be empty (in theorie)
    - basic blocks
    - statement
        - rvalues and operands
        - assign
        - etc
    - terminator
        - goto
        - switch int
        - call
        - etc
- panic handling
    - do not catch by other threads
    - distinguish between successful and unsuccessful termination
- which parts can or must be excluded from translation
    - std implementations
    - part between std::mutex and pthread mutex
- emulating features
    - missing mir parts
    - external libraries
    - intrinsics and platform specific behavior
    - mutexes
        - high level and low level interfaces
        - choosing high level
            - harder model checking for more general Mutexes
            - os independent
            - less elements need to be translated
                - smaller net
                - worse mapping
        - need to track associated locals

- joining the translations
    - joining basic elements
    - joining basic blocks
    - joining function calls
    - recursion limits
\end{verbatim}
\begin{figure}
    \centering
    \includegraphics[width=.9\textwidth]{../diagrams/basic_blocks.png}
    \caption{
        The structure of basic blocks and statements.
        Basic blocks as well as statements have a distinct start and end place.
        The start place of the first statement is identical with the basic block start place.
        The same is true for the lasts statements end place and the basic block end.
        Also the end place of a leading statement is the start place of the following statement (illustrating the sequential nature of statements).
        Terminators are represented entirely by transitions that are not pictured here, as they are typically linking to other basic blocks.
    }
    \label{basic_block_trans}
\end{figure}

\section{Petri Net Representation}
\label{app_petri}
\begin{verbatim}
    - petri net formalism and format (high level? edge descriptions)
\end{verbatim}


\section{Model Checking}
\label{app_mc}
\begin{verbatim}
- many tools
- pnml as common representation language
    - finding tools on standard site
- lola integration (format of a petri net)
  - own format
- CTL formula
    - expected deadlocks (program end)
    - difference between deadlock in petri net and deadlock in program
        - try to stay analogous
    - difference between p = 0 and EF( p = 0 )
    - try to force witness paths
\end{verbatim}

\section{Test Programs}
\label{app_test}
\begin{verbatim}
- stay simple to prove concept
    - empty program
    - programs which cover the basic language features
    - program with a deadlock
    - similar program without a deadlock
\end{verbatim}

\section{Debugging}
\label{app_debug}
\begin{verbatim}
- dot file for small programms
- finding unconnected nodes
    - possible bug found
    - marking live places as result
    - uninitialized places still have to be marked
      (or removed entirely)
- witness path
    - reducing the complicated net to witness path nodes 
      and its neighbors
\end{verbatim}
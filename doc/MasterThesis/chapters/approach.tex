% !TEX root = ../main.tex
\chapter{Approach}
\label{approach}
After we learned the underlying concepts, we can go on with the actual task.

In this chapter we will introduce an idea to translate Rust code into Petri-Nets.
We will see that the Rust compiler creates an intermediate representation that fits our needs,
we will work out a translation for each of the elements in this intermediate representation,
and discuss the format that we will translate into.
Finally, we will show how we can use a model checker to find a deadlock in a simple test program.

\section{Rust Compiler}
\label{app_rust}
There are basically two options to translate Rust into Petri-Nets:
\begin{enumerate}
    \item write a translator from scratch,
    \item or use something existing.
\end{enumerate}
Writing an own translator means total control over the process.
Features can be added iterative as needed and data structures can be designed efficiently for our special purpose.
However, this would result in a new compiler for Rust which eventually has to cover every language feature;
Including some difficult ones like macro expansion and generics.
Features that someone already implemented, spending a fair amount of thought in the process,
backed by a large community,
over several years.
Resulting in a compiler that is openly available under an open source license\cite{rustc}, with a maintained documentation\cite{rustc-guide}\cite{rustc-doc}.
If we want to use this compiler we have to learn how it works and which concepts are important.
This can be very time-consuming and difficult, but it seems to be time worth spend if we are able to use difficult features right away.
For this reason this project is based on the Rust compiler \textit{rustc}.
So let's see how its basic structure looks like.

The Rust compiler has several phases and a variety of intermediate representations \cite[Chapter 2.1]{rustc-guide}:
\begin{enumerate}
    \item In the first phase Rust source files are parsed to an abstract syntax tree (\textbf{AST}) that matches the original Rust syntax closely.
    \item In the second phase implicit information is expanded. This includes macros and identifier names.
    \item Phase three lowers the AST to a simpler form called high-level intermediate representation or \textbf{HIR}.
    The HIR still is quite similar to normal Rust syntax but some structures are normalized so that code analysis is easier. 
    For example for loops are rewritten to simple endless loops with break conditions in if-statements.
    \item The fourth phase executes some static analysis on the HIR.
    Things like type checking and encapsulation verification is done on this representation.
    \item Another representation is generated in phase five: the mid-level intermediate representation (\textbf{MIR}).
    Now we are leaving the tree structure and switching to a graph.
    MIR is based on a control-flow graph\cite{10.1145/800028.808479} and language features are reduced to a minimum.
    There is only one type of loop and branches respectively (only gotos instead of ifs or pattern matching).
    Additional static analysis like Rusts borrow checking as well as further optimization is done on the MIR level.
    \item Phase six lowers the MIR to the Rust independent LLVM\cite{lattner2004llvm} intermediate representation.
    LLVM IR is a low level representation that is close to assembly language.
    Additional optimization are done for this representation and the result is translated into several object files.
    \item Finally, in the last phase the object files are linked into a complete binary executable.
\end{enumerate}
This leaves us with several options to intercept and translate the current intermediate representation into Petri-Nets.

\section{Interception strategy}
\label{app_intercept}
After deciding to use the Rust compiler as basis for this work, we need to determine the phase we want to intercept the default compilation to translate to our own target.
A suitable location makes use of most existing compiler features and minimizes the own translation effort.

We want to intercept after basic features like name resolution as we need them ourselves and surely won't improve them in a reimplementation.
Also, code generation, including macro and generics expansion, should be handled by the stock compiler.
They just produce more Rust code that we will treat anyway.
Additionally, after expansion, no code generation syntax will remain, so we do not have to deal with it at all.

More optional for our use are the static analysis features like type checking and borrow checking.
Though, we want to use their assumptions, we do not depend on them actually being checked, since nobody will ever run a Rust program that did not go through the full compilation process.
So it is quite safe to assume that nobody will do model checking on a program that won't compile.
However, we still can enforce those invariants if we run the static analysis anyway and abort on errors.
This prevents time-consuming runs for programs that cannot be build.

A less optional compiler feature that will greatly ease our effort, is the lowering of the representation.
This way we can exploit that several high level language features are reduced to significantly less low level features.
Which, in turn, means fewer features we have to cover with our implementation.
We don't want to go to low though, since lower representations might prevent us from exploiting assumptions from the higher level representations.
We also probably want to avoid machine dependent representations.
After all we want to verify generic Rust programs and not the peculiarities of a single machine.
This might be debatable though.

Finally, we want to make use of all optimizations we can.
Especially optimizations that reduce the code (and net) size.
So features like constant propagation and dead code elimination would be nice to have to reduce the impact of state explosion.

Reminding the Rust compiler phases with those thoughts, the best place we can intercept is between phase five and six:
after borrow checking and optimizations on the MIR.
Here, all code was expanded, all Rust specific static analysis and optimization was done and all unnecessary language features where reduced to a minimal set of instructions.
And since both MIR and Petri-Nets are graph representations, a mapping should be relatively straight forward.
More so if we consider that MIR represents control flow quite closely.
Also, we avoid the even lower level and Rust independent LLVM IR and the machine dependent object files. 

\section{Mid-level Intermediate Representation (MIR)}
\label{app_mir}
Now where we pinned down MIR as the intermediate representation to use, it is helpful to understand how it is structured.
Here is a simple Rust program:
\lstset{language=Rust, caption={A Rust program. And there is no deadlock! },label=rust-switch, frame=none, stepnumber=5, backgroundcolor=\color{verylightgray}}
\begin{lstlisting}
pub fn main() {
    let x = 5;              // x is an integer
    let _y = get_result(x); // call a function with x
}
// a function that takes an integer and returns an integer
pub fn get_result(x: usize) -> usize {
    // check the value of x
    match x {
        1 => 1, // if it is one: return one
        2 => 2, // if it is two: return two
        _ => 5, // if it is something else: return 5
    }
}
\end{lstlisting}
And this is a textual representation of the corresponding MIR generated by the compiler:
\lstset{language=Rust, caption={Generated MIR to program from listing \ref{rust-switch}},label=rust-switch-mir, frame=none, stepnumber=5, backgroundcolor=\color{verylightgray}}
\begin{lstlisting}
fn  main() -> () {
    // Declaration of locals.
    let mut _0: ();     // the return local: no return => null-tuple
    let _1: usize;      // local _1 has type usize
    let _2: usize;
    let mut _3: usize;  // local _3 is mutable so it can change its value
    bb0: {                  // BasicBlock 0: execution starts here
        StorageLive(_1);    // initialize local
        _1 = const 5usize;  // assign constant value 5 to local _1
        StorageLive(_2);
        StorageLive(_3);
        _3 = _1;            // assign value of local _1 to local _3
        // Call function get_result with local _3 as argument
        // _3 moves into the scope of the function.
        // After that it can not be used in this scope anymore.
        // Continue execution in BasicBlock1.
        _2 = const get_result(move _3) -> bb1
    }
    bb1: {               // BasicBlock1
        StorageDead(_3); // local is not used again in this function
        StorageDead(_2);
        StorageDead(_1);
        return; // return from main, program finished
    }
}
fn  get_result(_1: usize) -> usize {
    // Declaration of locals.
    // The argument _1 is an additional local that can be used.
    let mut _0: usize; // same return type as in the function declaration
    bb0: {  // BasicBlock 0: execution starts here
        // Continue depending on the value of _1.
        // If it is one continue in BasicBlock2.
        // If it is two continue in BasicBlock3.
        // otherwise continue in BasicBlock1
        switchInt(_1) -> [1usize: bb2, 2usize: bb3, otherwise: bb1];
    }
    bb1: {
        _0 = const 5usize; // assign the return value
        goto -> bb4;       // continue in BasicBlock4
    }
    bb2: {
        _0 = const 1usize;
        goto -> bb4;
    }
    bb3: {
        _0 = const 2usize;
        goto -> bb4;
    }
    bb4: { return; } // return to the calling function
}
\end{lstlisting}


As mentioned before, MIR is derived from control flow graphs\cite[chapter 2.17]{rustc-guide}.
Consequently, it can also be represented graphically as shown in figure \ref{switch-mir-graph}.
It consists of several \textbf{basic blocks} which are interconnected with directed edges.
Each basic block consist of any amount of non branching \textbf{statements} (dark blue in figure \ref{switch-mir-graph}) and a single -- possibly branching -- \textbf{terminator} (red in figure \ref{switch-mir-graph}).
All statements in a single basic block are executed sequentially.
Between those, no branching can occur in or out.
Only terminators can redirect the control flow.
They are the ones that introduce conditional execution (i.e. if-then-else constructs) or jumps to other basic blocks (including loops).

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{../diagrams/SwitchInt.png}
    \caption{
        Graphical MIR from listing \ref{rust-switch-mir}.
        }
    \label{switch-mir-graph}
\end{figure}

Data in MIR is represented as \textbf{locals} and \textbf{places} (not to be confused with Petri-Net places).
Places represent any kind of memory location, whereas locals are conceptually always located on the stack.
This means that a location can also be represented as a place, but places are not necessarily locations.
In our example all places are also locals.

Statements work on these data representations.
The most prominent type of statement is the assignment that assigns an \textbf{rvalue} derived from an expression to a memory location -- meaning a place (the \textbf{lvalue}).
For example the second statement in \textit{BB0} from \textit{main} is an assignment statement that assigns the constant value \textit{5} (a unary expression with only one constant operator) to local \textit{\_1}.
All other statements in our example are \textit{StorageLive} and \textit{StorageDead} statements.
Locals cannot be used before they were set live or after they were set dead -- except for the return local (always local \textit{\_0}) and function arguments (always the first locals after local \textit{\_0}).
They are logically linked to locals in the calling function where the initialization was already handled.

Terminators can direct control flow for example with a function-call-terminator (like the one in \textit{BB0} of \textit{main}.
These redirect control flow to a subroutine that is executed until a return-terminator is hit.
This one marks the end of a function and let the flow continue in the calling function.
The return terminator in the main function marks the end of the program and results in its successful termination.

Terminators can also split control flow as the \textit{SwitchInt} terminator in \textit{get\_result} shows.
This terminator redirects control flow depending on the value of local \textit{\_1}.
In the example this results in three possible paths in which the function can be traversed.
All of which join in a basic block with a single \textit{return} terminator.

\section{Translation}
\label{app_trans}
After we saw how the MIR graph is structured, we can try to find a translation to Petri-Nets. This will include some more details and edge cases we have to consider.

\subsection{Entry Point}
We will start with the most abstract view on our translation: the whole program.
A program is something that typically has a beginning and an ending.
We can model this with a \textbf{program start} and a \textbf{program end} place.
Depending on the program they are interconnected somehow.
The sole exception are programs with a diverging main function -- where the main function ends in an endless loop.
In this case the end place will not be connected to the graph (which will have no negative effect on the verification process).
In Rust a second end place for panics -- the \textbf{panic end} place -- is a helpful addition.
This place will be marked then the program terminated unsuccessfully after a panic was raised.
A circumstance that might be helpful to distinguish in verification runs.
Finally, the program start place needs to be marked with a token.
We will later see that this token also indicates the first statement to execute and that it virtually moves from statement to statement acting like a program counter (even though Petri-Net tokens do not move semantically but rather be consumed and produced).
Figure \ref{program_stub} shows the three resulting places.
\begin{figure}
    \centering
    \includegraphics[width=.4\textwidth]{../diagrams/basic_program_new.png}
    \caption{
        Features that every program shares: the start and end places.
    }
    \label{program_stub}
\end{figure}

Although these are all the basic features shared by every program, we still have to look a little closer on the semantics of the program start place.
This place is a bit ambiguous since, in practice, the main function is not what is executed first.
Usually programs have a `runtime' that is initialized before main() is called.
In the runtime  static memory and language specific features are initialized (among other things).
And even though low level languages like Rust or C have a small runtime, they still have one.
Now we have to decide if this runtime should be considered for Petri-Net translation.
After all, it is part of the finally executed binary.
On the other hand it is platform dependent code that is independent of the actual program semantics.
And there is another problem in starting in the pre-main code.
It turns out that the MIR for that part is not completely available in every compiler version.
It is possible to get the missing parts but it complicates the translation process unnecessarily.
Additionally, we previously already argued for a platform independent approach in chapter \ref{app_intercept}.
It would be inconsistent to detach from that agenda now.
So, because of these reasons we decided to skip the pre-main() code and start translating programs at the main method.

\subsection{Functions}
The next important parts of a program are the executed functions.
As in most other imperative languages, Rust programs have a main function that wrap all its functionality (excluding the runtime as discussed in the previous section).
The main function can call other functions that in turn can call additional functions, and so on.
When executing a program, the called functions are organized on a stack called `call stack'.
When a new function is entered, a new stack frame is pushed on the stack, including information like function arguments and local variables.
On leaving a function it is removed from the stack, deallocating memory the frame occupied.

In MIR, calling and returning from functions is done in a basic block terminator and can occur arbitrarily often.
Considering this behavior in a Petri-Net function model would be of no help, leaving just the start place as a feasible similarity between functions as pictured in figure \ref{basic_program}:
a function that is called always begins its execution there.
And it will always be identical to the start place of the first basic block.

Now we can almost model the first complete program: the empty program:
\lstset{language=Rust,caption={The empty program},label=empty-program, frame=none, stepnumber=5, backgroundcolor=\color{verylightgray}}
\begin{lstlisting}
pub fn main() {}
\end{lstlisting}
The translated Petri-Net for the empty program can be seen in figure \ref{basic_program}.
Its program start place is also the start place of the main function.
We anticipate the return terminator here (discussed in chapter \ref{terminators}) to connect program start and program end place.
The result is a net that consumes a token from the program start place and produces a token on the program end place.
Afterwards the net is in its terminal state.

\begin{figure}
    \centering
    \includegraphics[width=.7\textwidth]{../diagrams/function_and_empty_program.png}
    \caption{
        A conceptual view on a function on the left. The Empty program on the right.
    }
    \label{basic_program}
\end{figure}

So, from a modeling perspective functions are not the decisive abstraction.
But if we consider the translation process they get more important.
The rustc interface for MIR works per function.
This means that it is not possible (at least not obviously) to get the MIR from a whole program.
Just function per function.
A likely explanation for this design choice is that a lot of context information switches with functions.
For example every functions starts with basic block zero and increases the count for the following basic blocks.
Local variables are also indexed from zero on for each function, with some of them having a special meaning:
the first local is always the functions return value and it is followed by locals for all function arguments.
If we want to translate a program, we have to keep this structure in mind.

In our implementation we have done this virtually with an own call stack.
We traverse the program function by function.
Every time we encounter a function-call-terminator we try to translate the called function and return where we left after we are done.
In a new call the context is switched to the new variables and basic blocks, and the return semantics is stored.

However, this approach has some implications:
\begin{enumerate}
    \item If the same function is called multiple times in the program, it is also translated multiple times.
    Although, this can probably be avoided with an intelligent function cache this approach is sufficient for a proof of concept.
    \item The far more extensive implication are the voided recursion capabilities.
    If we encounter a recursive function with this strategy, the translation process will be trapped in an endless loop.
\end{enumerate}
Actually recursion is a feature that can never be achieved with low-level Petri-Nets, as its data model cannot be mapped properly.
How often a recursive function is called depends on the data it is called with and cannot be known at compile time.
In normal program execution a recursive function is just pushed on a new stack frame again and again, until it can be resolved (the base case can be called for all recursive levels) or the stack overflows.
Most relevant programs will be still executable.
But we cannot model this stack like behavior in low-level Petri-Nets because we cannot get additional memory (this would mean a dynamic set of places in a static graph).
All memory we have is the memory we know of at compile time.
And we cannot reuse the places of a function for different recursion levels as we cannot distinguish the tokens from the recursion levels.
We could just remodel each recursion level again and again to a fixed recursion depth.
However, this would impact the verification results, since a property might change for programs with different maximum recursion depths.

A solution to this problem can be high-level Petri-Nets, where we can distinguish between tokens.
There, we could reuse places from the same function by annotating tokens with the corresponding recursion level.
We won't go into detail of this approach, though, since this work is based on the low-level semantics.
We will just have to accept that we cannot translate recursive functions for the time being.

\subsection{Memory}
To translate a Rust program we cannot focus only on execution flow.
We also need a valid representation of data.
Unfortunately, as mentioned before, data is complicated to model in low-level Petri-Nets.
With only the concept of bare tokens, we can only model finite memory space and even then: modeling every state of every possible variable would bloat our net terribly.

To stay in the realm of low-level nets we need to abstract from variable states.
In principle a Petri-Net place is already something that holds data: a set of tokens.
For our purposes we can interpret a place as a memory location and a token as arbitrary data.
Though we loose the information for the actual variable value, we still can analyze the program flow.
Additionally, this representation resonates well with a possible high-level extension where the token can be annotated with the current value of a variable.
So, to keep it simple we represent a memory location as a single place with a single token.
`Reading' from and `writing' to memory can be modeled with a transition that consumes the token, while also producing a token.
In theory the consumed and produced token in a read has to be the same while writing can change the token.
But again this is only relevant for high-level nets.

Now, in many programming language there are different concepts of memory that behave differently, and Rust is no exception.
Basically we can distinguish between four different concepts\cite[Chapter 3.1, Chapter 4.1, Chapter 19.1]{klabnik2018rust}:
\begin{enumerate}
    \item \textbf{Constant memory space}: The size of the constant space is known at compile time and the values of this space do not change during runtime (hence the name).
    The values of all constants are compiled into the executable and no computation is needed to get them.
    \item \textbf{Static memory space}: Like in the constant space, the size of the static memory space is known at compile time.
    The big difference is that data in the static space can change during runtime.
    So during program startup, a fixed amount of memory is allocated and all static variables are initialized.
    Later they can be used rather similar to normal variables. With some Rust specific constrains that we will not discuss here, as they are not important for Petri-Net translation.
    \item The \textbf{Stack}: where the size is known for each stack frame (function call).
    This is the combined size of all local variables a function needs for execution.
    If a function is called a new frame is pushed to the stack and if the function terminates the frame is removed.
    Since the last pushed frame will be the first to be removed (LiFo - Last in, first out), the stack can be managed without data fragmentation.
    \item The \textbf{Heap}: which is the place there every remaining memory goes.
    A dynamic variable, there the size cannot be known at compile time needs to allocate memory on demand at runtime.
    If the memory is not needed anymore, it will be deallocated and can be reused later.
    Both allocation and deallocation are on demand, which can introduce memory fragmentation, since the size of the holes is not uniform.
\end{enumerate}

The most prominent concept on the MIR layer is the stack.
Every part of the MIR is associated with a function which has a set of local variables (locals).
The current state of a local variable is changed by statements.
This includes the values the variable can be assigned to as well as information about variable liveness.
Each local starts in an uninitialized state until a statement is called to set it `storage live'.
A living variable can be assigned to arbitrary values (constraint by its type) as long as it is needed.
Afterwards a statement sets the variable `storage dead' to indicate that it will not be used again in this function call.
We can model these states with three Petri-Net places for each local as shown in figure \ref{local}: one place for the uninitialized state, one for the living state there the variable can be used and one for the dead state.
The first active state will be `uninitialized' so this one must be marked with a token initially.
The unique `storage live' and `storage dead' statements (which are special statements generated for the MIR) have to be called to traverse the three states.
They will consume a token from the previous state and produce one on the next.
This enforces that data access can only be done if a token is on the live place.
This way we can later verify if the liveness invariants are met.
\begin{figure}
    \centering
    \includegraphics[width=.2\textwidth]{../diagrams/local.png}
    \caption{Places of a local in a function.}
    \label{local}
\end{figure}

Heap and static space is hidden behind local variables.
For example, we can access a value on the heap by dereferencing a pointer we stored in a local.
In MIR this is done with a projection from a local that describes which part of the local is used:
which field of a struct, which index of an array or if we dereference the local.
This information is stored implicitly as MIR-place on which statements can operate.
Unfortunately this projection is hard to impossible to model.
Especially pointer dereferencing is a problem since the memory model is handled by the operating system at runtime.
But for all these projections the source -- the initial local -- is known.
If a projection is used in a statement we can interpret this as access to the initial local.
Again we loose some information here in exchange for a manageable design.

Much easier to model is the space for constant variables.
Their behavior is close to locals that live for the entire runtime.
So they do not need any uninitialized or dead place, just a place that represents the current value.
In fact, we can model the whole constant space as a single Petri-Net place, where every access is done with different transitions.
There is no data manipulation anyway.
And again this approach resonates well with a possible high-level Petri-Net extension where every accessing transition produces tokens annotated with the corresponding constant values.

In conclusion, we need two models for memory access: a single place that represents the whole constant memory space and a set of places for each local variable with uninitialized, live, and dead place.
If we have to handle heap space, we can hide the access behind a local variable (ignoring the projection).

\subsection{Basic Blocks}
A basic block is essentially a container for a sequential part of a program; 
With an entry point and an exit point.
On exit, program flow can be redirected to one or more other basic blocks (redirections will again start at the entry point of a basic block).

Consequently, in a Petri-Net model, a basic block has an entry place and an exit place.
These are bound to the containing statements, where the basic block's entry place corresponds to the first statement's entry place and the basic block's exit place corresponds to the last statement's exit place.
The terminator is modeled by one or more transitions that consume the token from the end place and produce a token on another basic blocks start place.

\subsection{Statements}
\begin{figure}
    \centering
    \includegraphics[width=.9\textwidth]{../diagrams/basic_blocks.png}
    \caption{
        The structure of basic blocks and statements.
    }
    \label{basic_block_trans}
\end{figure}
Statements are the part of the MIR graph that change data.
And there are several kinds of them, with some of them not used in every compiler phase.
For example a MIR statement named `FakeRead' is used for static analysis but is removed in a later optimization phase when the sanity was satisfied.

However, the general structure of statements, is always rather similar.
In Petri-Net terms, they have a starting place, a transition that manipulates some data places and an end place.
If two statements succeed each other, the first statements end place will be the seconds statements start place.
Similarly, the start place of the first statement in a sequence will be shared with the start place of the surrounding basic block.
Likewise, the end place of the basic block is shared with the end place of the last statement.
Figure \ref{basic_block_trans} illustrates this connection.
The statement transition will consume a token from the start place and produce a token on its end place.
Additionally, the transition will consume and produce a token on data places to resemble an interaction with this data.
As mentioned earlier the actual data is transparent in the low-level Petri-Net model but might be added in a possible high-level model.

As an example for the data manipulation we can look at the three most important statements\footnote{A complete list can be found in the documentation:\newline
https://doc.rust-lang.org/nightly/nightly-rustc/rustc/mir/enum.StatementKind.html}: `StorageLive', `StorageDead' and `Assign'.
\begin{itemize}
    \item The \textit{StorageLive} statement consumes a token from the \textit{uninitialized} place of a local variable, and produces one on its \textit{live} place.
    \item The \textit{StorageDead} statement consumes a token from the \textit{live} place of a local variable, and produces one on its \textit{dead} place.
    \item An \textit{Assign} statement in the language semantics, evaluates an expression and assigns the result to a memory location (lvalue).
    In Petri-Net semantics it consumes a token from the lvalues live place and all live places that are involved in the rvalue expression.
    Simultaneously new tokens are produced on all of them (one per involved place).
    This means that no assign statement can be executed until the corresponding locals are initialized with a storage live statement or after the local was retired with a storage dead statement.
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=.6\textwidth]{../diagrams/StatementsNet.png}
    \caption{Three kinds of statements (unconnected)}
    \label{statements_net}
\end{figure}

With the virtual movement of a token from a statement's start place to another we modeled a marking mechanism for the currently active instruction (meaning a statement or terminator).
If we would simulate the resulting Petri-Net, we always have a token on exactly one start place of a single instruction.
This instruction will be the next one to be executed and afterwards a new one will be marked.
A property that is very close to a \textit{program counter} in CPU's which stores the address of the instruction that will be executed next.
This next instruction might just increment the counter (in sequential parts) to mark the subsequent instruction as active or manipulate it to jump to a totally different instruction.
This is a convenient similarity since it increases confidence that the Petri-Net actually models something similar to an execution semantics of a real program.

\subsection{Terminators}
\label{terminators}
After we covered the strict sequential part of our program we will now discuss the jumping and branching terminators.
They too have a common structure:
Every terminator has at least one transition which consumes a token from a basic blocks end place and produce a token on a basic blocks start place.

The MIR representation again has different kinds of terminators\footnote{A complete list can be found in the documentation: https://doc.rust-lang.org/nightly/nightly-rustc/rustc/mir/enum.TerminatorKind.html}. Here is a list with the most important ones:
\begin{itemize}
    \item The most basic \textit{Goto} terminator has a single transition.
    It connects two basic blocks inside a single function.
    \item The \textit{SwitchInt} terminator implements a conditional branch to multiple other basic blocks inside a single function.
    A branching path can only be taken if its condition is met.
    Only the last branching path acts as an \textit{otherwise} branch.
    It will be taken if there is no other valid path where the condition is met.
    Since the Petri-Net representation is used for model checking, we always consider every possible branch.
    As a result we can ignore the condition and just connect all involved basic blocks with a transition.
    \item The \textit{Call} terminator connects basic blocks of different functions.
    It also encodes function arguments and the basic block to continue after the called function returns.
    We have to remember this information for the translation (especially the arguments);
    And have to make sure that we reuse their locals in the called function.
    Also, function calls might panic.
    In that case execution continues on a separate basic block.
    \item The \textit{Return} terminator is the return path from a successful function call.
    Here the current basic block is connected with the one that is encoded in the Call terminator.
    \item The \textit{Resume} terminator is the return path from an unsuccessful function call.
    This terminator is connected with the panic basic block that is encoded in the function call.
    \item The \textit{Assert} terminator branches depending on a condition.
    If the condition evaluates as expected -- normal execution flow continues, and if it does not -- a panic is started on a separate path.
    Again, in the Petri-Net we always consider both cases, so we can safely ignore the condition and just model both paths with a transition.
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=.8\textwidth]{../diagrams/TerminatorsNet.png}
    \caption{Connection of terminator transitions.}
    \label{terminators_net}
\end{figure}

With an entry point, basic blocks, a representation of locals and constants, statements and terminators we can build a more complex program.
Consider this simple program with a function call:
\lstset{language=Rust,caption={A simple function call},label=function_call_program, frame=none, stepnumber=5, backgroundcolor=\color{verylightgray}}
\begin{lstlisting}
pub fn main() {
    let mut x = 5;
    x = call(x);
}

fn call(i: usize) -> usize {
    i * 2
}
\end{lstlisting}
The generated MIR for listing \ref{function_call_program} (shown in listing \ref{function_call_program_mir}) is a bit more complex.
Not only because of the \textit{StorageLive} and \textit{StorageDead} statements, but also because of additional copies of locals.
Additionally, both \textit{main} function and \textit{call} function are split into two basic blocks.
The \textit{main} function has to be split because the function-call-terminator redirects control flow to another function.
And the \textit{call} function has to be split because of a multiplication that might overflow.
An assert-terminator guarantees that this behavior is detected and treated.

\lstset{language=Rust,caption={Generated MIR for a simple function call},label=function_call_program_mir, frame=none, stepnumber=5, backgroundcolor=\color{verylightgray}}
\begin{lstlisting}
    fn  main() -> () {
        let mut _0: ();
        let mut _1: usize;
        let mut _2: usize
        let mut _3: usize
        bb0: {
            StorageLive(_1);
            _1 = const 5usize;
            StorageLive(_2);
            StorageLive(_3);
            _3 = _1;
            _2 = const call(move _3) -> bb1;
        }
    
        bb1: {
            StorageDead(_3);
            _1 = move _2;
            StorageDead(_2);
            StorageDead(_1);
            return;
        }
    }
    fn  call(_1: usize) -> usize {
        let mut _0: usize; 
        let mut _2: usize; 
        let mut _3: (usize, bool); 
        bb0: {
            StorageLive(_2); 
            _2 = _1; 
            _3 = CheckedMul(move _2, const 2usize 
            assert(!move (_3.1: bool), "attempt to multiply with overflow") -> bb1 
        }
        bb1: {
            _0 = move (_3.0: usize); 
            StorageDead(_2); 
            return; 
        }
    }
    
\end{lstlisting}

Figure \ref{function_call_net_pruned} shows how the translated net should look for the \textit{main} function.
The program starts in \textit{BB0}, where all locals are set \textit{StorageLive} and the value of local \textit{\_1} is set to 5.
At the end of the basic block, the subroutine is called with local \textit{\_2} as return value and local \textit{\_3} as an argument.
To represent that their values might change in the subroutine they are connected with it.
After the call returns, its return value is assigned to local \textit{\_1} and all locals are set \textit{StorageDead}.
Only the panic place is unconnected since there is no panic path generated in the MIR representation.

\begin{figure}
    \centering
    \includegraphics[width=.95\textwidth]{../diagrams/FunctionCallNetPruned.png}
    \caption{Main function for listing \ref{function_call_program}}.
    \label{function_call_net_pruned}
\end{figure}


But it could as well have been.
And since this is an important part of program execution it demands a closer look.

\section{Panic Handling}
Error handling is a vital part in programming and typically is considered in program language design.
Many languages implement an exception semantic with try and catch blocks.
Exceptions can be thrown and if they are not caught the program terminates ungracefully.

Rust has a different approach: normally functions are expected not to fail.
Functions that can fail will return a \textit{Result} type that can hold the result of the computation or an error with a description.
The type system enforces handling of both cases and the language gives some mechanisms to do so.
Of course the interesting part is the error case.
This one can be escalated to previous function calls until a consistent program state can be restored.

But it might not always be possible to recover from an error state.
In such a situation the program can be instructed to \textit{panic} and shutting down ungracefully like with an uncaught exception in other languages.
The program execution is aborted, the stack will be unwound and an error message with the details of the panic is generated (stating the error message and the location of the error).
Though panics can be caught, by a parent thread they typically lead to the termination of the current program.
This panic structure ensures that the compiler always knows when a panic can happen so it generates appropriate code.

Code that we can identify and handle in the MIR representation.
We hinted this in the last chapter: some terminators can branch execution to a normal path or a panic path.
Branches to a panic path lead to basic blocks that are marked as cleanup and handle stack unwinding.
If execution enters a cleanup path, it cannot return to a normal path and will end up in an erroneous termination (unless caught by a parent thread).
We modeled this event with the separate `panic end' place in our Petri-Net.
However, stack unwinding will execute generated code that is not a distinct part of the program semantic.
After all, what really matters is that an erroneous state was reached that we can't recover.
So we can reduce the size of our net by skipping the cleanup paths and directly put a mark on the panic place.

Of course the Rust compiler cannot cast black magic to prevent the really awful errors like dereferencing pointers to inaccessible memory.
Such a miscalculation would lead directly to an OS-exception (or other undefined behavior) causing an error that cannot be caught by Rust -- or worse -- a messed up memory state with no error at all!
And since Rust cannot detect those errors, we cannot model them either.
We just have to assume that all compiled operations are valid in the execution context.

This kind of errors have to be avoided by the implementer.
But Rust aids avoidance by marking such operations and functions as \textbf{unsafe} to use.
Most of the time it is possible to avoid unsafe code using abstractions and safe operations.
Only a small code base needs to use unsafe code to create those safe abstractions. 
And once the integrity there is kept, all depending higher level code benefits.

\section{Interface Emulation}
\label{emulation}
There are endless possibilities to implement an algorithm and not all of them depend on Rust as description language.
Still, using established algorithms from other languages is always a desirable feature to have.
Unsurprisingly, Rust implements some interfaces to access functionality that is not native to itself.
The two most important ones are compiler \textit{intrinsic functions} where the implementation is hidden by the compiler;
And the \textit{Foreign Function Interface} (FFI) for interfacing C-Style exports.
Both of them call instructions that are not represented in the MIR graph.

These calls leave the realm of Rust code where no guarantees can be made (which makes most of them unsafe).
But on lower level code we won't get around them.
For example Rusts thread interface in Linux systems is an abstraction of the `pthread' library which is written in C.
So, if we use threads, we use foreign code.
To translate such items we will have to emulate them somehow.
In case of compiler intrinsics this would be a finite amount of work but there is no upper bound for foreign functions that might be called.
So we have to implement at least a generic translation that works for all calls.

A generic FFI-call can be thought of as a combination of a statement and a terminator.
The locals for the arguments have to be accessed by a transition like we did for statements.
Afterwards we branch depending on the return value, like we did in the terminators.
This model is sufficient to describes all basic data manipulation functionality.
But there are other functionalities that we need to address in a special way:
some foreign functionality can influence the execution flow.
For example mutexes that are vital for our purposes.

\subsection*{Mutexes}
Mutexes guard execution flow depending on a data variable.
If we just access the variable and continue like we do for statements, flow could always continue.
However, flow should only be allowed to continue if the mutex variable can be acquired (and block otherwise).
This can easily be modeled in Petri-Nets with a marked mutex place as figure \ref{mutex_net} shows.
Then, on every try to acquire the mutex a transition has to consume the token from the mutex place.
The first try will do so without further problems but every following attempt will be blocked, since transitions can only fire when all preplaces are properly marked.
When leaving the critical section a transition needs to reproduce a mark on the mutex place again, so that blocked processes can continue with their computation.
\begin{figure}
    \centering
    \includegraphics[width=.4\textwidth]{../diagrams/mutexNet.png}
    \caption{Two flows guarded by a mutex.}
    \label{mutex_net}
\end{figure}

In Rust mutexes are part of the standard library.
Before data guarded by a mutex can be mutated, it has to be unlocked.
If it is, the data cannot be accessed again until the lock is released.
This is done automatically when the variable of the accessed data goes out of scope.
Listing \ref{rust-mutex} shows an example usage of a mutex.

\lstset{language=Rust,caption={Mutex in Rust},label=rust-mutex, frame=none, stepnumber=5, backgroundcolor=\color{verylightgray}}
\begin{lstlisting}
use std::sync::{Arc, Mutex};

// Here we're using an Arc (reference counting smart pointer)
// to share memory among threads, and the data inside the Arc
// is protected with a mutex.
let data = Arc::new(Mutex::new(0));
{ // begin of a new scope
    // Mutex is acquired by the lock() method.
    // If the lock was previously acquired the call will block
    // until the lock is released
    let mut data: Arc<Mutex<i32>>= *data.lock().unwrap();
    // mutate data
    data += 1;
    // the lock is released here when `data` goes out of scope.
} // end of scope
\end{lstlisting}

Now, the call to mutex \textit{lock()} and the release of the lock will trigger several other functions that handle a thread safe access and the blocking behavior.
For translation, we can model the complete underlying behavior or abstract from it.
Both approaches are perfectly valid: the former would be closer to the actual execution behavior while the latter would be closer to the basic program semantics (that does not care about the mutex implementation).
But in favor of a small Petri-Net, we stick to the abstract way that ignores the implementation.
When a mutex is called we will stall normal translation and just insert our own abstraction at the place of the corresponding function calls.
But there is more we have to consider.

If we take a look on a pruned version of the MIR layer in listing \ref{rust-mutex-mir}, we can identify a Problem.
Because mutexes are unlocked implicitly and often are wrapped in other types, we loose the information when a specific mutex is locked or unlocked.
The mutex that is created in \textit{bb0} is wrapped in a smart pointer in \textit{bb2} masking the original mutex.
In this simple program, that smart pointer is unnecessary but usually mutexes guard critical sections to ensure thread safety.
However, to reference the mutex safely in multiple threads the reference has to be thread safe, which is ensured by the wrapping \textit{Arc} smart pointer.
By the time we lock the mutex in \textit{bb4} we can not read directly from the local which mutex it belongs to (in case there are multiple mutexes in a program).
The same problem is inherited by the mutex guard in local \textit{\_3}.

\lstset{language=Rust, caption={Pruned MIR for using a mutex},label=rust-mutex-mir, frame=none, stepnumber=5, backgroundcolor=\color{verylightgray}}
\begin{lstlisting}

fn  main() -> () {
    let mut _0: ();
    let _1: Arc<Mutex<i32>>;
    let mut _2: Mutex<i32>;
    let mut _4: &Mutex<i32>;
    let _5: &Mutex<i32>;
    let mut _6: &Arc<Mutex<i32>>;
    scope 1 {
        let _3: Result<MutexGuard<i32>, PoisonError<MutexGuard<i32>>>; 
    }
    // create the mutex first
    bb0: {
        _2 = const Mutex::<i32>::new(const 0i32) -> bb2; 
    }
    // move the mutex into the smart pointer
    bb2: {
        _1 = const Arc::<Mutex<i32>>::new(move _2) -> bb3; 
    }
    // dereference the smart pointer
    bb3: {
        _6 = &_1;                        
        _5 = const <Arc<Mutex<i32>> as deref(move _6) 
             -> [return: bb4, unwind: ...]; 
    }
    // lock the mutex
    bb4: {
        _4 = _5;                         
        _3 = const Mutex::<i32>::lock(move _4) 
             -> [return: ..., unwind: ...]; 
    }
    // mutex goes out of scope and destructor is called
    bb6: {
        drop(_3) -> [return: ..., unwind: ...];
    }
}
\end{lstlisting}

To connect our transitions to the correct mutex places we have to track or infer the corresponding mutexes when they are locked or unlocked.
In theory the strict borrowing and aliasing rules of Rust should ensure that the correct mutex can always be inferred.
However, this is not trivial in our model; So in the test implementation the local variables that are associated with a mutex (like the Arc in local \_1 and the MutexGuard in local \_3) are marked with the original mutex (in a separate data structure).
Locking and unlocking mutexes then just check the marking for the fitting mutex and connect its mutex places with the correct transitions.
This is a simple approach for a proof of concept, but it probably can be improved, especially in terms of storage consumption.
